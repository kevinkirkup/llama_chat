{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a40be9",
   "metadata": {},
   "source": [
    "# Langchain Agent testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb16380",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107d1b4",
   "metadata": {},
   "source": [
    "### Initialize Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06edbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9e0b0",
   "metadata": {},
   "source": [
    "### Initialize the Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddecf97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/utils/utils.py:157: UserWarning: WARNING! temporature is not default parameter.\n",
      "                temporature was transferred to model_kwargs.\n",
      "                Please confirm that temporature is what you intended.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/utils/utils.py:157: UserWarning: WARNING! n_gqa is not default parameter.\n",
      "                n_gqa was transferred to model_kwargs.\n",
      "                Please confirm that n_gqa is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from /Users/kevinkirkup/ai/models/llama2/llama-2-70b-chat.Q5_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.3.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:            blk.3.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:         blk.3.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:              blk.3.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:            blk.5.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:         blk.5.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.10.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.10.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.10.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:             blk.10.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:        blk.10.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.10.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:             blk.10.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.11.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.11.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.11.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.11.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.5.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.5.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:            blk.6.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.6.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.6.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.6.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:         blk.6.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.6.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.6.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.7.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.7.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:              blk.7.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.7.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:         blk.7.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.7.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.7.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.8.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.8.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:              blk.8.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.8.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:         blk.8.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:              blk.8.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:              blk.8.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:            blk.9.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:            blk.9.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:              blk.9.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:              blk.9.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:         blk.9.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:              blk.9.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:              blk.9.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:           blk.11.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.11.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.12.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:           blk.12.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:        blk.12.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:             blk.12.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.17.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:        blk.17.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:        blk.21.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:             blk.21.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.22.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.26.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:        blk.26.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:           blk.28.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:        blk.28.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.30.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:        blk.30.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:             blk.30.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.31.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:        blk.31.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:             blk.31.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.32.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:           blk.32.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:             blk.32.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:        blk.32.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:             blk.32.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.33.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:           blk.33.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:             blk.33.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:        blk.33.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:             blk.33.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.34.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:        blk.34.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:           blk.34.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:             blk.34.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.35.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.35.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:             blk.35.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:        blk.35.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:             blk.35.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.36.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.36.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:             blk.36.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:        blk.36.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:             blk.36.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.37.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:           blk.37.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:             blk.37.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:        blk.37.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:             blk.37.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.38.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:           blk.38.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:             blk.38.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:        blk.38.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:             blk.38.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.39.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:           blk.39.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:             blk.39.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:        blk.39.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:             blk.39.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.40.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:           blk.40.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:             blk.40.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:        blk.40.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:             blk.40.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.41.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:           blk.41.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:             blk.41.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:        blk.41.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:             blk.41.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.42.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:           blk.42.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:             blk.42.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:        blk.42.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:             blk.42.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.43.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:           blk.43.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:             blk.43.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:        blk.43.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:             blk.43.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.44.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:           blk.44.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:             blk.44.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:        blk.44.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:             blk.44.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:           blk.45.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:             blk.45.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:        blk.45.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:             blk.45.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.46.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:           blk.46.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:             blk.46.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:        blk.46.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:             blk.46.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.47.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:           blk.47.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:             blk.47.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:        blk.47.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:             blk.47.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.48.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:           blk.48.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:             blk.48.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:        blk.48.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:             blk.48.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.49.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:           blk.49.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:             blk.49.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:        blk.49.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:             blk.49.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.50.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:           blk.50.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:             blk.50.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:        blk.50.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:             blk.50.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:           blk.51.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:             blk.51.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:        blk.51.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:             blk.51.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.52.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:           blk.52.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:             blk.52.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:        blk.52.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:             blk.52.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.53.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:           blk.53.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:             blk.53.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:        blk.53.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:             blk.53.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.54.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:           blk.54.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:             blk.54.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:        blk.54.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:             blk.54.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.55.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:           blk.55.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:             blk.55.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:        blk.55.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:             blk.55.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.56.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:           blk.56.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:             blk.56.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:        blk.56.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:             blk.56.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:             blk.57.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:        blk.57.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:           blk.57.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:             blk.57.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.58.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:           blk.58.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:             blk.58.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:        blk.58.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:             blk.58.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.59.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:           blk.59.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:             blk.59.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:        blk.59.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:             blk.59.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.60.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:           blk.60.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:             blk.60.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:        blk.60.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:             blk.60.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.61.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:           blk.61.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:             blk.61.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:        blk.61.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:             blk.61.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.62.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:           blk.62.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:             blk.62.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:        blk.62.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:             blk.62.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.63.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:           blk.63.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:             blk.63.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:        blk.63.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:             blk.63.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.64.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:           blk.64.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:             blk.64.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:        blk.64.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:             blk.64.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.65.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:           blk.65.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:             blk.65.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:        blk.65.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:             blk.65.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.66.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:           blk.66.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:             blk.66.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:        blk.66.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:             blk.66.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.67.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:           blk.67.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:             blk.67.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:        blk.67.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:             blk.67.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:           blk.68.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:             blk.68.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:        blk.68.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:             blk.68.attn_v.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q5_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.69.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:           blk.69.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:             blk.69.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:        blk.69.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:             blk.69.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.70.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:           blk.70.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:             blk.70.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:        blk.70.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:             blk.70.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.71.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:           blk.71.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:             blk.71.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:        blk.71.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:             blk.71.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.72.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:           blk.72.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:             blk.72.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:        blk.72.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:             blk.72.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.73.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:           blk.73.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:             blk.73.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:        blk.73.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:             blk.73.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:           blk.74.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:             blk.74.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:        blk.74.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:             blk.74.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.75.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:           blk.75.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:             blk.75.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:        blk.75.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:             blk.75.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.76.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:           blk.76.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:             blk.76.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:        blk.76.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:             blk.76.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.77.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:           blk.77.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:             blk.77.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:        blk.77.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:             blk.77.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.78.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:           blk.78.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:             blk.78.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:        blk.78.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:             blk.78.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.79.ffn_down.weight q6_K     [ 28672,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:           blk.79.ffn_gate.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.ffn_up.weight q5_K     [  8192, 28672,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:             blk.79.attn_k.weight q5_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:        blk.79.attn_output.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:             blk.79.attn_q.weight q5_K     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.attn_v.weight q6_K     [  8192,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q5_K:  481 tensors\n",
      "llama_model_loader: - type q6_K:   81 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = mostly Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 68.98 B\n",
      "llm_load_print_meta: model size       = 45.40 GiB (5.65 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.23 MB\n",
      "llm_load_tensors: mem required  = 46494.72 MB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  640.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 313.88 MB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 36864.00 MB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  9836.31 MB, offs =  38439649280, (46700.94 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   642.00 MB, (47342.94 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   308.02 MB, (47650.95 / 49152.00)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "model_path = os.path.expanduser(\"~/ai/models/llama2/llama-2-70b-chat.Q5_K_M.gguf\")\n",
    "model = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    temporature=0,\n",
    "    n_gqa=8,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e36aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 485 tensors from /Users/kevinkirkup/ai/models/sqlcoder/sqlcoder.Q5_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_K     [  6144, 49152,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:             position_embd.weight q5_K     [  6144,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:             blk.0.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:            blk.0.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:                blk.0.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.0.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.0.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:           blk.1.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:             blk.1.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:         blk.1.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.1.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.1.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:                blk.1.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.1.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:              blk.1.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.2.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.2.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.2.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:         blk.2.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:           blk.2.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.2.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.2.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:              blk.2.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:                blk.2.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.2.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.2.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.3.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:             blk.3.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:            blk.3.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.3.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.3.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.3.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.3.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.3.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.3.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:                blk.3.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:            blk.3.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.3.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.4.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.4.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.4.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.4.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:         blk.4.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.4.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.4.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.4.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.4.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:                blk.4.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.4.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:              blk.4.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.5.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.5.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:            blk.5.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.5.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:         blk.5.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:           blk.5.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.5.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.5.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:              blk.5.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:                blk.5.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.5.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.5.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.6.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:             blk.6.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:            blk.6.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.6.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.6.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.6.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.6.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.6.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.6.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:                blk.6.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:            blk.6.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.6.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.7.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.7.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.7.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:              blk.7.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:         blk.7.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.7.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.7.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:              blk.7.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:              blk.7.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:                blk.7.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:            blk.7.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:              blk.7.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.8.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.8.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:            blk.8.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:              blk.8.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:         blk.8.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:           blk.8.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:            blk.8.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:              blk.8.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:              blk.8.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:                blk.8.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:            blk.8.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.8.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.9.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.9.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:            blk.9.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:              blk.9.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:         blk.9.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.9.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:            blk.9.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.9.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:              blk.9.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:                blk.9.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:            blk.9.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:              blk.9.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:          blk.10.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:            blk.10.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.10.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.10.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:        blk.10.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.10.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.10.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.10.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.10.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:               blk.10.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.10.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.10.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.11.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:            blk.11.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:           blk.11.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.11.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:        blk.11.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:          blk.11.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.11.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.11.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.11.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:               blk.11.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.11.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.11.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:          blk.12.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:            blk.12.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:           blk.12.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.12.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.12.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.12.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.12.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.12.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:                    output.weight q6_K     [  6144, 49152,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.12.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:               blk.12.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:           blk.12.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.12.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:          blk.13.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:            blk.13.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.13.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.13.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:        blk.13.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:          blk.13.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.13.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.13.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.13.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:               blk.13.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.13.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.13.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:          blk.14.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:            blk.14.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.14.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.14.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.14.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:          blk.14.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.14.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.14.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.14.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:               blk.14.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.14.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.14.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:          blk.15.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:            blk.15.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.15.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.15.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.15.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.15.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.15.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.15.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.15.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:               blk.15.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:           blk.15.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.15.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:          blk.16.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:            blk.16.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.16.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.16.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:        blk.16.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:          blk.16.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.16.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.16.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.16.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:               blk.16.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.16.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.16.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:          blk.17.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:            blk.17.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.17.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.17.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.17.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:          blk.17.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.17.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.17.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.17.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:               blk.17.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:           blk.17.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.17.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:          blk.18.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:            blk.18.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.18.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.18.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.18.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.18.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.18.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.18.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.18.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:               blk.18.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.18.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.18.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:          blk.19.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:            blk.19.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.19.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.19.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:        blk.19.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.19.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.19.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.19.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.19.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:               blk.19.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.19.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.19.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:          blk.20.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:            blk.20.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.20.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.20.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.20.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:          blk.20.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.20.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.20.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.20.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:               blk.20.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:           blk.20.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.20.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:          blk.21.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:            blk.21.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.21.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.21.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.21.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.21.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.21.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.21.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.21.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:               blk.21.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.21.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.21.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:          blk.22.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:            blk.22.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.22.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.22.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:        blk.22.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.22.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.22.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.22.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.22.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:               blk.22.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.22.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:             blk.22.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:          blk.23.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:            blk.23.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.23.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.23.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.23.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:          blk.23.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.23.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.23.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:             blk.23.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:               blk.23.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:           blk.23.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:             blk.23.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:          blk.24.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:            blk.24.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:           blk.24.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:             blk.24.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:        blk.24.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:          blk.24.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.24.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.24.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:             blk.24.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:               blk.24.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:           blk.24.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.24.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:          blk.25.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:            blk.25.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.25.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.25.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:        blk.25.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:          blk.25.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:           blk.25.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.25.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.25.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:               blk.25.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.25.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:             blk.25.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:          blk.26.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:            blk.26.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.26.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.26.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:        blk.26.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:          blk.26.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:           blk.26.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.26.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:             blk.26.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:               blk.26.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:           blk.26.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:             blk.26.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:          blk.27.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:            blk.27.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:           blk.27.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:             blk.27.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:        blk.27.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:          blk.27.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.27.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.27.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:             blk.27.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:               blk.27.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:           blk.27.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.27.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:          blk.28.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:            blk.28.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.28.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.28.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:        blk.28.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:          blk.28.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:           blk.28.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.28.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.28.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:               blk.28.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.28.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:             blk.28.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:          blk.29.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:            blk.29.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.29.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.29.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:        blk.29.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:          blk.29.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:           blk.29.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:             blk.29.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:             blk.29.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:               blk.29.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:           blk.29.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:             blk.29.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:          blk.30.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:            blk.30.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:           blk.30.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:             blk.30.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:        blk.30.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:          blk.30.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:           blk.30.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:             blk.30.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:             blk.30.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:               blk.30.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:           blk.30.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.30.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:          blk.31.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:            blk.31.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.31.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.31.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:        blk.31.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:          blk.31.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:           blk.31.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.31.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.31.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:               blk.31.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.31.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:             blk.31.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:          blk.32.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:            blk.32.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.32.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.32.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:        blk.32.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:          blk.32.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:           blk.32.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:             blk.32.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:             blk.32.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:               blk.32.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:           blk.32.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:             blk.32.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:          blk.33.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:            blk.33.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:           blk.33.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:             blk.33.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:        blk.33.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:          blk.33.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:           blk.33.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:             blk.33.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:             blk.33.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:               blk.33.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:           blk.33.ffn_down.weight q5_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.33.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:          blk.34.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:            blk.34.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.34.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.34.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:        blk.34.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:          blk.34.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:           blk.34.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.34.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.34.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:               blk.34.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.34.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:             blk.34.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:          blk.35.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:            blk.35.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.35.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.35.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:        blk.35.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:          blk.35.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:           blk.35.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:             blk.35.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:             blk.35.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:               blk.35.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:           blk.35.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:             blk.35.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:          blk.36.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:            blk.36.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:           blk.36.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:             blk.36.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:        blk.36.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:          blk.36.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:           blk.36.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:             blk.36.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:             blk.36.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:               blk.36.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:           blk.36.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.36.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:          blk.37.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:            blk.37.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.37.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.37.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:        blk.37.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:          blk.37.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:           blk.37.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.37.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.37.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:               blk.37.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.37.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:             blk.37.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:          blk.38.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:            blk.38.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.38.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.38.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:        blk.38.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:          blk.38.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:           blk.38.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:             blk.38.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:             blk.38.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:               blk.38.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:           blk.38.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:             blk.38.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:          blk.39.attn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:            blk.39.attn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:           blk.39.attn_qkv.weight q6_K     [  6144,  6400,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:             blk.39.attn_qkv.bias f32      [  6400,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:        blk.39.attn_output.weight q5_K     [  6144,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:          blk.39.attn_output.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:           blk.39.ffn_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:             blk.39.ffn_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:             blk.39.ffn_up.weight q5_K     [  6144, 24576,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:               blk.39.ffn_up.bias f32      [ 24576,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:           blk.39.ffn_down.weight q6_K     [ 24576,  6144,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.39.ffn_down.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:               output_norm.weight f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:                 output_norm.bias f32      [  6144,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                   starcoder.context_length u32     \n",
      "llama_model_loader: - kv   3:                 starcoder.embedding_length u32     \n",
      "llama_model_loader: - kv   4:              starcoder.feed_forward_length u32     \n",
      "llama_model_loader: - kv   5:                      starcoder.block_count u32     \n",
      "llama_model_loader: - kv   6:             starcoder.attention.head_count u32     \n",
      "llama_model_loader: - kv   7:          starcoder.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   8:     starcoder.attention.layer_norm_epsilon f32     \n",
      "llama_model_loader: - kv   9:                          general.file_type u32     \n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q5_K:  102 tensors\n",
      "llama_model_loader: - type q6_K:   61 tensors\n",
      "llm_load_print_meta: format           = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch             = starcoder\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 6144\n",
      "llm_load_print_meta: n_head           = 48\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 48\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 15B\n",
      "llm_load_print_meta: model ftype      = mostly Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 15.82 B\n",
      "llm_load_print_meta: model size       = 10.74 GiB (5.83 BPW) \n",
      "llm_load_print_meta: general.name   = StarCoder\n",
      "llm_load_print_meta: BOS token = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token  = 203 '\n",
      "'\n",
      "llm_load_tensors: ggml ctx size =    0.16 MB\n",
      "llm_load_tensors: mem required  = 11000.77 MB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =   40.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: loading '/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x117db0760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x117db0c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x117db10e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x117db1330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x117db1580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x117db17d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x117db1a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x117db1c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x117db1ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_4                     0x117db2110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x117db2360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                0x117db25b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                   0x117db2800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x117db2a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x117db2ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x117db2ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x117db3140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x117db3390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x117db35e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x117db3830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x117db3a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x117db3cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x117db3f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x117db4170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f32_f32                0x117db43c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x117db4610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x117db4860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x117db4ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x117db4d00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x117db4f50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x117db51a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x117db53f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x117db5640 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x117db5890 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x117db5ae0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x117db5d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                 0x117db5f80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x117db61d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x117db6420 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x117db6670 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x117db68c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x117db6b10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x117db6d60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x117db6fb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x117db7200 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x117db7450 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_f32                       0x117db76a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_f16                       0x117db78f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x117db7b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x117db7d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x117db7fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x117db8230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 49152.00 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 225.88 MB\n",
      "llama_new_context_with_model: max tensor size =   236.25 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 11002.44 MB, (58653.39 / 49152.00), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =    42.00 MB, (58695.39 / 49152.00), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   220.02 MB, (58915.41 / 49152.00), warning: current allocated size is greater than the recommended max working set size\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "sql_model_path = os.path.expanduser(\"~/ai/models/sqlcoder/sqlcoder.Q5_K_M.gguf\")\n",
    "#sql_model_path = os.path.expanduser(\"~/ai/models/sqlcoder/sqlcoder.Q5_K_M.gguf\")\n",
    "sql_model = LlamaCpp(\n",
    "    model_path=sql_model_path,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    temporature=0,\n",
    "    n_gqa=8,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0feaaa",
   "metadata": {},
   "source": [
    "### Create an agent to fetch the users in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00d7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"postgresql://mercury:m3ssenger@localhost:5432/mercury_dev\",\n",
    "    include_tables=['users', 'roles', 'roles_users', 'orgs', 'ports', 'port_groups', 'virtual_routers', 'links', 'connections'],\n",
    "    sample_rows_in_table_info=3,\n",
    ")\n",
    "toolkit = SQLDatabaseToolkit(\n",
    "    db=db,\n",
    "    llm=sql_model,\n",
    "    verbose=True\n",
    ")\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=model,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b455879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input:\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mconnections, links, orgs, port_groups, ports, roles, roles_users, users, virtual_routers\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 21528.24 ms\n",
      "llama_print_timings:      sample time =    13.46 ms /    18 runs   (    0.75 ms per token,  1337.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7424.44 ms /   176 tokens (   42.18 ms per token,    23.71 tokens per second)\n",
      "llama_print_timings:        eval time =  3079.56 ms /    17 runs   (  181.15 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:       total time = 10542.96 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m Now that I know the table names, I can query their schemas to see what columns they have.\n",
      "Action: sql_db_schema\n",
      "Action Input: roles\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE roles (\n",
      "\tid UUID NOT NULL, \n",
      "\tname TEXT NOT NULL, \n",
      "\tdescription TEXT, \n",
      "\torg_id UUID, \n",
      "\tinserted_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, \n",
      "\tupdated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, \n",
      "\tCONSTRAINT roles_pkey PRIMARY KEY (id), \n",
      "\tCONSTRAINT roles_org_id_fkey FOREIGN KEY(org_id) REFERENCES orgs (id) ON DELETE CASCADE\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from roles table:\n",
      "id\tname\tdescription\torg_id\tinserted_at\tupdated_at\n",
      "da391bd2-be50-4d3b-9349-eb5c1d81f355\tnetwork:user\tDefault role for a user of network resources.\n",
      "\tNone\t2022-03-18 19:28:37.425921\t2022-03-18 19:28:37.425922\n",
      "0852cb83-f206-4b88-b8cf-1d33917e3578\torg:maintainer\tDefault role for maintainer of an Organization\n",
      "\tNone\t2022-03-18 19:28:37.425900\t2022-03-18 19:28:37.425901\n",
      "bbee079a-2035-4dda-bb29-0a6a1b120da0\torg:admin\tDefault role for administrator of an Organization.\n",
      "\tNone\t2022-03-18 19:28:37.425874\t2022-03-18 19:28:37.425875\n",
      "*/\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 21528.24 ms\n",
      "llama_print_timings:      sample time =    27.77 ms /    39 runs   (    0.71 ms per token,  1404.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   830.75 ms /    30 tokens (   27.69 ms per token,    36.11 tokens per second)\n",
      "llama_print_timings:        eval time =  6908.78 ms /    38 runs   (  181.81 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:       total time =  7816.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m Great, now that I know what columns are available in the roles table, I can query it to find users with the org:admin role.\n",
      "Action: sql_db_query_checker\n",
      "Action Input: \n",
      "SELECT * FROM roles WHERE name = 'org:admin';\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 21528.24 ms\n",
      "llama_print_timings:      sample time =    47.61 ms /    63 runs   (    0.76 ms per token,  1323.22 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11729.88 ms /   492 tokens (   23.84 ms per token,    41.94 tokens per second)\n",
      "llama_print_timings:        eval time = 12384.84 ms /    62 runs   (  199.76 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time = 24254.89 ms\n",
      "\n",
      "llama_print_timings:        load time = 17781.92 ms\n",
      "llama_print_timings:      sample time =   270.26 ms /   256 runs   (    1.06 ms per token,   947.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 17781.85 ms /   140 tokens (  127.01 ms per token,     7.87 tokens per second)\n",
      "llama_print_timings:        eval time = 11566.22 ms /   255 runs   (   45.36 ms per token,    22.05 tokens per second)\n",
      "llama_print_timings:       total time = 30073.10 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "```sql\n",
      "SELECT avg(amount_of_money), sum(profit)\n",
      "FROM payments natural join sales\n",
      "  where status = 'COMPLETE' and site_id = 12345678;\n",
      "```\n",
      "\"\"\"\n",
      "\n",
      "# %%\n",
      "#Importing the libraries needed to run this script\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "#Defining the function that calculates the average amount of money and total profit for each sale. The function takes in a dataframe as input, which should contain the following columns: payment_id, price, paid_amount, net_profit_margin, site_id. The function returns two dataframes; one with the averaged amount of money per sale and another with the total profit from all completed sales.\n",
      "def getAverageAndTotalProfit(df):\n",
      "    df['average_amount'] = round(df['price']/100*df['paid_amount'],2)\n",
      "    \n",
      "    avgAmntPerSale = df[['sale_id','average_amount']].groupby(['sale_id']) \\\n",
      "        .agg({'average_amount':'mean'}) \\\n",
      "        .reset_index()\n",
      "    totalProfit = df[df.status == 'COMPLETE'][['sale_id', '\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now that I know what columns are available in the roles table, I can query it to find users with the org:admin role.\n",
      "Action: sql_db_query\n",
      "Action Input: \n",
      "SELECT * FROM roles WHERE name = 'org:admin';\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(UUID('bbee079a-2035-4dda-bb29-0a6a1b120da0'), 'org:admin', 'Default role for administrator of an Organization.\\n', None, datetime.datetime(2022, 3, 18, 19, 28, 37, 425874), datetime.datetime(2022, 3, 18, 19, 28, 37, 425875)), (UUID('bf75dc62-79d9-4087-aa95-7b94c10320fb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('567cd9c5-28df-49f7-b09c-34f1125dd4a1'), datetime.datetime(2023, 7, 20, 17, 42, 56, 277096), datetime.datetime(2023, 7, 20, 17, 42, 56, 277096)), (UUID('fb4dd7ba-ba28-47fa-9aeb-bd33be888d5f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('34771142-9fce-4c90-9756-f2d544ed81f5'), datetime.datetime(2022, 8, 16, 16, 46, 42, 439231), datetime.datetime(2022, 8, 16, 16, 46, 42, 439235)), (UUID('9ef3fdc2-5901-4f67-9043-2ab7800aa80a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f281f083-4b3f-4cde-8c4c-b9d6504404aa'), datetime.datetime(2023, 7, 28, 18, 48, 12, 69997), datetime.datetime(2023, 7, 28, 18, 48, 12, 69997)), (UUID('54e58cdf-abd3-4053-98fe-26f24a611e72'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f8929dd7-a23c-451c-9bfa-5eeb3b1eb0e4'), datetime.datetime(2022, 8, 16, 16, 46, 42, 467653), datetime.datetime(2022, 8, 16, 16, 46, 42, 467656)), (UUID('4bdfb694-9a8b-4279-8bb1-55d176b58051'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ead33762-ddf6-4182-96d5-6e170c2003ba'), datetime.datetime(2022, 8, 16, 16, 46, 42, 529197), datetime.datetime(2022, 8, 16, 16, 46, 42, 529200)), (UUID('7a48f9e0-880e-4254-9f46-3a12e378c025'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e0d85c34-1c06-41d9-8638-aac877371f5b'), datetime.datetime(2022, 8, 16, 16, 46, 42, 550937), datetime.datetime(2022, 8, 16, 16, 46, 42, 550940)), (UUID('7a18655a-f7bf-43f3-970e-430cca8ab55f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a350c99b-58a2-4d1e-b9e9-a128f4e45370'), datetime.datetime(2023, 7, 28, 18, 48, 15, 229791), datetime.datetime(2023, 7, 28, 18, 48, 15, 229791)), (UUID('d96dc40a-d1ef-4793-aa7c-18dd763f74c9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a3c1d91a-e932-434a-a459-1a8c8aec29e0'), datetime.datetime(2022, 8, 16, 16, 46, 42, 615534), datetime.datetime(2022, 8, 16, 16, 46, 42, 615537)), (UUID('2fd14bbc-84e1-4be2-9c51-e429765cbc3d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f375a4ed-097b-4277-aa8f-0f023966436a'), datetime.datetime(2022, 8, 16, 16, 46, 42, 638716), datetime.datetime(2022, 8, 16, 16, 46, 42, 638720)), (UUID('03d13326-cf03-4b9c-8cdd-14a9760fc86d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('da6024f9-77be-478e-977d-a36aa8cc06ae'), datetime.datetime(2022, 8, 16, 16, 46, 42, 660681), datetime.datetime(2022, 8, 16, 16, 46, 42, 660684)), (UUID('5e270143-4a3d-4356-9fc1-448e30cc0ce2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('497ffdb3-0668-40d9-bcb2-e7f3c408a534'), datetime.datetime(2022, 8, 16, 16, 46, 42, 726464), datetime.datetime(2022, 8, 16, 16, 46, 42, 726467)), (UUID('0609c5c3-8a2e-4e3f-b209-fe6549bc63a5'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5a2cfe31-06f7-46b2-86bd-ee5719b4fce8'), datetime.datetime(2022, 8, 16, 16, 46, 42, 749358), datetime.datetime(2022, 8, 16, 16, 46, 42, 749361)), (UUID('79cba169-604d-464d-abfe-6c789159bc78'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5fd90d25-57f9-4379-af29-43dbc8e36eac'), datetime.datetime(2023, 7, 13, 13, 16, 24, 648904), datetime.datetime(2023, 7, 13, 13, 16, 24, 648904)), (UUID('6501694b-39e2-4f2f-9055-80276ccaefe1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c4e4b493-b409-4512-9e1d-7354b2270d4d'), datetime.datetime(2022, 8, 16, 16, 46, 42, 814573), datetime.datetime(2022, 8, 16, 16, 46, 42, 814576)), (UUID('b5d65e85-2392-4408-927f-f9c11aacb40e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('bec90f6e-feb3-4a48-a4a8-3437612c349b'), datetime.datetime(2022, 8, 16, 16, 46, 42, 836894), datetime.datetime(2022, 8, 16, 16, 46, 42, 836897)), (UUID('879b2b6e-008f-4c73-a425-a5226694c564'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('46f259c4-c8ff-4e54-a241-56f5eddf3aa7'), datetime.datetime(2022, 8, 16, 16, 46, 42, 858038), datetime.datetime(2022, 8, 16, 16, 46, 42, 858042)), (UUID('3bb90fef-4dbe-42e4-87aa-951220209b6a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4f4adb9c-f5d8-4801-a980-4894f9fb84bf'), datetime.datetime(2022, 8, 16, 16, 46, 42, 938366), datetime.datetime(2022, 8, 16, 16, 46, 42, 938371)), (UUID('a5722107-2a1b-449b-bc46-3e232d33a6e0'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e4595331-9576-4ed6-ac41-2d9be17fd2a3'), datetime.datetime(2023, 7, 14, 15, 29, 23, 347384), datetime.datetime(2023, 7, 14, 15, 29, 23, 347384)), (UUID('247380df-f6af-47cd-bb33-56f57375825b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('58d615db-85b4-48c9-bb12-da3e4d513174'), datetime.datetime(2023, 7, 11, 14, 50, 14, 678117), datetime.datetime(2023, 7, 11, 14, 50, 14, 678117)), (UUID('55343b9f-92b8-48ee-8110-f97bf8e18210'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('273f8531-e0fd-40a4-8228-925dc81a5e4c'), datetime.datetime(2022, 8, 16, 16, 46, 42, 971455), datetime.datetime(2022, 8, 16, 16, 46, 42, 971474)), (UUID('e683403c-e8d8-4b47-b227-475f6646960f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('029294d1-ddcd-4d8c-98a3-aea8afa1b735'), datetime.datetime(2022, 8, 16, 16, 46, 43, 28428), datetime.datetime(2022, 8, 16, 16, 46, 43, 28432)), (UUID('8e558b49-802b-4d0a-a2e6-81e34f475841'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d22b3e3a-c7ed-4f73-84ce-c9d011bcaa61'), datetime.datetime(2022, 8, 16, 16, 46, 43, 51079), datetime.datetime(2022, 8, 16, 16, 46, 43, 51083)), (UUID('bb1360dc-1188-4b33-9db0-0a6f7598d1a3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('01abbd4a-37f0-4897-84c1-012287017910'), datetime.datetime(2022, 8, 16, 16, 46, 43, 116287), datetime.datetime(2022, 8, 16, 16, 46, 43, 116290)), (UUID('3bebd2f8-8ece-4e24-830b-77b7a69d1dd8'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f9ac3ed9-f500-4115-afbb-81fc2fbb0a41'), datetime.datetime(2023, 7, 14, 16, 23, 38, 279153), datetime.datetime(2023, 7, 14, 16, 23, 38, 279153)), (UUID('6251d4f4-d416-4a6a-9667-f61307ae4218'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d6949cee-1744-4fd4-87ec-3997f6d9ae52'), datetime.datetime(2022, 8, 16, 16, 46, 43, 138330), datetime.datetime(2022, 8, 16, 16, 46, 43, 138333)), (UUID('8a112a74-b17a-46cf-aa77-fa9ae48fcd57'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('b1bd86cf-410a-4328-8847-8cf9d6415272'), datetime.datetime(2022, 8, 16, 16, 46, 43, 160433), datetime.datetime(2022, 8, 16, 16, 46, 43, 160436)), (UUID('b886540c-6d3f-42bc-9c09-955bf2057848'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('85b0c613-5125-4c55-b369-5279d92330e8'), datetime.datetime(2022, 8, 16, 16, 46, 43, 233904), datetime.datetime(2022, 8, 16, 16, 46, 43, 233908)), (UUID('056bf023-7428-4d66-ab48-2e86714d9f59'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3e0a59fa-1376-4a6c-b983-f35bbaf6820c'), datetime.datetime(2022, 8, 16, 16, 46, 43, 261559), datetime.datetime(2022, 8, 16, 16, 46, 43, 261563)), (UUID('d304ea28-daa5-431d-bb12-02b2b2acf143'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('931e032f-db7b-40cd-a521-59fd92554f5d'), datetime.datetime(2022, 8, 16, 16, 46, 43, 319165), datetime.datetime(2022, 8, 16, 16, 46, 43, 319168)), (UUID('faec1549-ab87-43e0-b735-2bf804eece87'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a0792122-0aea-41bb-8969-bb01391f969f'), datetime.datetime(2022, 8, 16, 16, 46, 43, 342110), datetime.datetime(2022, 8, 16, 16, 46, 43, 342113)), (UUID('c8373a94-c5da-4e12-a05d-d11c54196766'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1f84cdcb-c6a3-4424-9aaa-f079d17e7429'), datetime.datetime(2022, 8, 16, 16, 46, 43, 364681), datetime.datetime(2022, 8, 16, 16, 46, 43, 364684)), (UUID('60dcdb76-8369-48d5-b57f-5a4d957b4463'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('51e4e5b3-c41a-4284-b54e-f397c6a8d021'), datetime.datetime(2022, 9, 7, 19, 34, 52, 122797), datetime.datetime(2022, 9, 7, 19, 34, 52, 122797)), (UUID('d0b3ca00-d984-4d4c-abf8-2a3bbb6a75ff'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c7a86ffc-9e19-4cc9-ae48-e617a8d28af2'), datetime.datetime(2022, 9, 7, 19, 34, 52, 655407), datetime.datetime(2022, 9, 7, 19, 34, 52, 655407)), (UUID('4326064c-6789-4a31-a8c9-9958aa9e0fd6'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f6d8764b-6e30-401d-ae20-5e87f5a01c5b'), datetime.datetime(2022, 10, 3, 18, 9, 55, 264332), datetime.datetime(2022, 10, 3, 18, 9, 55, 264332)), (UUID('21845612-b9cc-4d6c-8286-47e9ae5e7c7a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5a476185-1861-4938-ada1-4f7886ed0350'), datetime.datetime(2022, 10, 4, 15, 38, 36, 295495), datetime.datetime(2022, 10, 4, 15, 38, 36, 295495)), (UUID('471c5939-9181-4d5c-9d9f-1593b4ad1682'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e59c1d9f-e835-471c-aa51-b9c8ad6d40bb'), datetime.datetime(2022, 10, 7, 14, 37, 32, 903376), datetime.datetime(2022, 10, 7, 14, 37, 32, 903376)), (UUID('d880b0e5-1acb-4425-afc0-b6312a98a2bc'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3f0b6222-7dca-402a-af0b-54c9bc781ce7'), datetime.datetime(2022, 10, 18, 14, 40, 59, 87967), datetime.datetime(2022, 10, 18, 14, 40, 59, 87967)), (UUID('1b147e3c-4977-4973-8bd9-6bd1650cff46'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('88bc9be3-a9bd-48e9-a6f3-42eda08aea40'), datetime.datetime(2022, 10, 21, 15, 14, 3, 640134), datetime.datetime(2022, 10, 21, 15, 14, 3, 640134)), (UUID('9849bcb5-be93-4178-8b4c-9d6b93a56f3b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a71caa8a-2037-4561-b03c-c549b4c7700f'), datetime.datetime(2022, 10, 21, 15, 14, 4, 67229), datetime.datetime(2022, 10, 21, 15, 14, 4, 67229)), (UUID('fb381776-caca-4fd4-aa5b-9e55c386d7e1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2486a84a-e41e-4b80-bded-d96ed342d34f'), datetime.datetime(2022, 10, 25, 15, 6, 43, 599699), datetime.datetime(2022, 10, 25, 15, 6, 43, 599699)), (UUID('e93344d1-cfbe-47e3-96ce-725af97fb384'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('850c65ff-880f-43c9-aa9a-5d8b57accf93'), datetime.datetime(2022, 10, 25, 15, 6, 44, 117549), datetime.datetime(2022, 10, 25, 15, 6, 44, 117549)), (UUID('381c8756-901c-43da-bbb3-ff817f9098e6'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d78dfd08-dfd4-4a5c-856a-ba1a8a58bc36'), datetime.datetime(2022, 10, 25, 16, 7, 1, 662544), datetime.datetime(2022, 10, 25, 16, 7, 1, 662544)), (UUID('b02db5b1-6d02-4ec1-9606-b39a40742010'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('10633aa7-d115-4a9b-b2bf-e2cba58ab122'), datetime.datetime(2022, 11, 10, 17, 58, 7, 232550), datetime.datetime(2022, 11, 10, 17, 58, 7, 232550)), (UUID('53806530-91f6-4e91-9ae5-d2db61e5150a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('79d4b2d1-bc48-4e74-85f0-56388f9bbb43'), datetime.datetime(2022, 11, 15, 19, 33, 38, 831746), datetime.datetime(2022, 11, 15, 19, 33, 38, 831746)), (UUID('b5f9b2c3-6b0c-4038-be62-5e23914800a1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a3ac58e8-7f39-47ed-8224-27126037b55d'), datetime.datetime(2023, 7, 13, 13, 13, 55, 523357), datetime.datetime(2023, 7, 13, 13, 13, 55, 523357)), (UUID('8aa25dd0-f33c-4d5d-bc95-8b186202cac8'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d68c7d26-7d9a-4b5a-a6e6-9d27ca2a6edc'), datetime.datetime(2022, 11, 28, 15, 54, 40, 704149), datetime.datetime(2022, 11, 28, 15, 54, 40, 704149)), (UUID('2989d8d5-d5e7-477a-8689-7b1b8f7055f9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('575eb5a5-bb10-443b-b841-d64f191d9778'), datetime.datetime(2022, 12, 1, 16, 39, 27, 707659), datetime.datetime(2022, 12, 1, 16, 39, 27, 707659)), (UUID('d7e09faf-5857-4501-9464-2cfe0a756da4'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d1e7d3ac-5b0c-4a00-b3ab-6904f39b627b'), datetime.datetime(2022, 12, 7, 23, 24, 23, 858702), datetime.datetime(2022, 12, 7, 23, 24, 23, 858702)), (UUID('d2192e23-1a36-45f8-8676-7e5b46a718b2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('15a48c4d-53f2-442e-8a21-cf14d5905311'), datetime.datetime(2022, 12, 8, 18, 48, 9, 175880), datetime.datetime(2022, 12, 8, 18, 48, 9, 175880)), (UUID('6a3cad7b-d5f2-4764-9a50-8025fc457a17'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f8fc7183-facf-4d39-a347-95a196699b66'), datetime.datetime(2022, 12, 12, 16, 9, 24, 74913), datetime.datetime(2022, 12, 12, 16, 9, 24, 74913)), (UUID('5c782a86-eebb-4c7c-b2ef-6758e2b5ea21'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('328bc9b5-0b0d-4455-9fe1-3937f1c1c479'), datetime.datetime(2022, 12, 12, 16, 9, 24, 575302), datetime.datetime(2022, 12, 12, 16, 9, 24, 575302)), (UUID('1e8b8b36-7044-4820-b581-607e7bc4bd1b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1435fdea-8063-4361-937e-7e06c3d0ecce'), datetime.datetime(2022, 12, 20, 16, 33, 32, 164789), datetime.datetime(2022, 12, 20, 16, 33, 32, 164789)), (UUID('147741f2-4103-4642-990b-8775fb1ba9b8'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('db832051-ec75-4487-85be-c6e59928caad'), datetime.datetime(2022, 12, 28, 15, 56, 43, 733092), datetime.datetime(2022, 12, 28, 15, 56, 43, 733092)), (UUID('8e7ca265-bc13-4782-99e4-eb2ebf59cf19'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('852a5d1d-cbe8-4f93-aa54-e48ba63fb92c'), datetime.datetime(2022, 12, 28, 15, 56, 44, 206031), datetime.datetime(2022, 12, 28, 15, 56, 44, 206031)), (UUID('479b10ad-498a-46d2-ac28-4f8b4277e9e9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f9e5773b-b2e3-42ca-91c8-5dd4c9031bc3'), datetime.datetime(2023, 1, 11, 17, 18, 55, 268987), datetime.datetime(2023, 1, 11, 17, 18, 55, 268987)), (UUID('1db5737c-9ed0-4dd1-8323-de06fe9c9319'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ef7e5891-0015-4c71-b360-a01cc7d1c6da'), datetime.datetime(2023, 1, 13, 20, 31, 36, 745979), datetime.datetime(2023, 1, 13, 20, 31, 36, 745979)), (UUID('ae251492-3e43-4991-8d6e-087de5813b46'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4b838907-adb5-434c-9a93-3eea41886709'), datetime.datetime(2023, 1, 17, 17, 7, 21, 961077), datetime.datetime(2023, 1, 17, 17, 7, 21, 961077)), (UUID('dd6db320-ffd8-46a6-9959-aae75cff94c7'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4b97296a-d95f-49da-8674-5bd4eebbe5d5'), datetime.datetime(2023, 2, 7, 21, 58, 53, 96295), datetime.datetime(2023, 2, 7, 21, 58, 53, 96295)), (UUID('dc013a09-9bab-44eb-8041-d0a5cdd4c7a8'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('42d775c1-324f-4565-8a89-bd9de45cd4ae'), datetime.datetime(2023, 2, 7, 21, 58, 53, 580910), datetime.datetime(2023, 2, 7, 21, 58, 53, 580910)), (UUID('a1ee7e8d-4641-46ed-a8e5-3b58a9f263a9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a20873ab-ceff-4b76-9340-10d40bc53109'), datetime.datetime(2023, 2, 10, 14, 29, 48, 687115), datetime.datetime(2023, 2, 10, 14, 29, 48, 687115)), (UUID('acc431bf-8a2f-44c4-a277-06b998c223a2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7de96e78-0c1a-4e5c-a25b-85f4f5d2f7e6'), datetime.datetime(2023, 2, 10, 19, 9, 58, 446657), datetime.datetime(2023, 2, 10, 19, 9, 58, 446657)), (UUID('16c35f30-4e59-4b39-98c7-58676ad0b10a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2a3eeecf-7d64-4218-9409-742b5198598a'), datetime.datetime(2023, 2, 10, 20, 0, 21, 981011), datetime.datetime(2023, 2, 10, 20, 0, 21, 981011)), (UUID('8b1493f6-994d-4a16-880f-d8505e61e056'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1849431a-7194-4d6a-b33c-681b6212fecf'), datetime.datetime(2023, 2, 17, 13, 29, 15, 201595), datetime.datetime(2023, 2, 17, 13, 29, 15, 201595)), (UUID('e3f36893-4d5e-4287-a83a-a951b6aa5008'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ad9d6e83-efe1-43cf-914c-ecb708e22e30'), datetime.datetime(2023, 2, 22, 20, 14, 8, 783532), datetime.datetime(2023, 2, 22, 20, 14, 8, 783532)), (UUID('a6132a8d-5aab-4ab2-a50a-e79f9c0a508f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('075d8972-d09e-482d-805f-c216fe8e9ee2'), datetime.datetime(2023, 2, 22, 20, 14, 9, 273048), datetime.datetime(2023, 2, 22, 20, 14, 9, 273048)), (UUID('ec39f299-d4e1-4f6a-a346-3cb6d0a582e2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7924a432-3c33-4f7c-979a-ca238754f24f'), datetime.datetime(2023, 2, 22, 20, 19, 0, 434148), datetime.datetime(2023, 2, 22, 20, 19, 0, 434148)), (UUID('5ecbc921-db2d-4854-a834-4fea4cafc6ff'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('9d497c11-ee95-4ca5-9726-aaf2384e2de4'), datetime.datetime(2023, 2, 22, 20, 19, 0, 879372), datetime.datetime(2023, 2, 22, 20, 19, 0, 879372)), (UUID('3f806e5c-c916-4328-9b07-747218a62864'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7488b759-666a-4203-b49c-a960a9c53511'), datetime.datetime(2023, 2, 24, 17, 34, 34, 479350), datetime.datetime(2023, 2, 24, 17, 34, 34, 479350)), (UUID('86a6d145-e7c7-4133-a63d-b710978ac5e5'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('b2cee178-7f59-4e6f-935c-4c17355daca5'), datetime.datetime(2023, 3, 2, 14, 47, 39, 186113), datetime.datetime(2023, 3, 2, 14, 47, 39, 186113)), (UUID('205d1e72-c452-48d8-986a-fd2fff1fc1f3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4b35e11c-614a-4b2e-be92-c4bc410f753c'), datetime.datetime(2023, 3, 6, 17, 38, 35, 255135), datetime.datetime(2023, 3, 6, 17, 38, 35, 255135)), (UUID('4c676c86-e466-4606-84e7-a75f613b725b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('b9a147b0-a4ca-4b63-a3b8-6702727effd3'), datetime.datetime(2023, 3, 7, 18, 18, 5, 447110), datetime.datetime(2023, 3, 7, 18, 18, 5, 447110)), (UUID('a58f4b7e-f773-4aab-ab70-636599fd1dde'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('830f6d4f-4332-45f7-9064-17f3bb04f7c5'), datetime.datetime(2023, 3, 8, 18, 22, 12, 119201), datetime.datetime(2023, 3, 8, 18, 22, 12, 119201)), (UUID('043f43a3-6b0e-42c0-9ffe-b4789a1e19f2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4e28b141-7c91-4880-bebb-c1dd3c5057a8'), datetime.datetime(2023, 3, 10, 17, 11, 8, 162254), datetime.datetime(2023, 3, 10, 17, 11, 8, 162254)), (UUID('84ab47a4-03a8-4987-b921-121d131a1153'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('bc199ae0-cfd3-4a3e-92d5-2829b2da8b9c'), datetime.datetime(2023, 3, 14, 23, 41, 42, 688473), datetime.datetime(2023, 3, 14, 23, 41, 42, 688473)), (UUID('70b78103-b2f7-41e4-90a2-033353c632f5'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7d100ab4-0da7-4cd5-ac64-43a86c3b252f'), datetime.datetime(2023, 3, 15, 22, 18, 44, 703163), datetime.datetime(2023, 3, 15, 22, 18, 44, 703163)), (UUID('29622895-d838-40ae-921a-a34975433510'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ce2c902d-bb8f-432c-a596-53d39918a26a'), datetime.datetime(2023, 3, 16, 14, 6, 50, 671312), datetime.datetime(2023, 3, 16, 14, 6, 50, 671312)), (UUID('b95125cc-0627-4175-9c71-af925436d0ce'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('245dd81f-32c3-406b-a34d-93a7eecd7f2a'), datetime.datetime(2023, 3, 16, 19, 5, 43, 48188), datetime.datetime(2023, 3, 16, 19, 5, 43, 48188)), (UUID('7b75b682-08c2-4665-a84a-c674faa3d9e9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('0a386b62-1c81-4834-94b5-91ea6f3b7a0c'), datetime.datetime(2023, 3, 17, 14, 37, 32, 461054), datetime.datetime(2023, 3, 17, 14, 37, 32, 461054)), (UUID('87b3cd97-ba5a-40d4-8c1b-694ad7c8c76d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('68eef4f1-5e60-44de-846a-078e57802dcb'), datetime.datetime(2023, 3, 23, 15, 22, 25, 688540), datetime.datetime(2023, 3, 23, 15, 22, 25, 688540)), (UUID('8b684e4c-7b1f-4c79-9001-77057917916c'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('9cf1c2b9-bf3e-4204-ba42-abf0ab3876ab'), datetime.datetime(2023, 3, 23, 15, 22, 26, 380993), datetime.datetime(2023, 3, 23, 15, 22, 26, 380993)), (UUID('28619a33-beb2-4829-9227-607afa519bc6'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('738b6894-86ec-47f0-9ae4-0cd08c1f5f00'), datetime.datetime(2023, 3, 23, 15, 22, 26, 926943), datetime.datetime(2023, 3, 23, 15, 22, 26, 926943)), (UUID('c78a4871-86e3-4d63-a0ea-b773bf11ec8e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c38caa28-c71f-4d7c-a8bb-c26f9fcff831'), datetime.datetime(2023, 3, 24, 14, 23, 37, 803424), datetime.datetime(2023, 3, 24, 14, 23, 37, 803424)), (UUID('e91d387c-baf4-419d-ad85-2b5c1ff9f307'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7e90ca94-c8b7-4e22-86c0-2cd17f9eafba'), datetime.datetime(2023, 3, 24, 14, 23, 38, 398065), datetime.datetime(2023, 3, 24, 14, 23, 38, 398065)), (UUID('fc02a784-22b3-4c24-86a6-9d0e03dad441'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3db56e99-9f77-4911-916d-640bea439551'), datetime.datetime(2023, 3, 28, 14, 19, 17, 140223), datetime.datetime(2023, 3, 28, 14, 19, 17, 140223)), (UUID('75b8373f-3fc6-4953-b304-11d8f1420180'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('dfd14c91-0514-4d97-bdf0-18874363e844'), datetime.datetime(2023, 3, 28, 14, 51, 24, 181605), datetime.datetime(2023, 3, 28, 14, 51, 24, 181605)), (UUID('8e160938-f315-46c4-a6ab-9c91b36427e6'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('eb124261-2877-48dc-9fb2-9054a13da4ff'), datetime.datetime(2023, 3, 29, 15, 26, 25, 735280), datetime.datetime(2023, 3, 29, 15, 26, 25, 735280)), (UUID('6caedfc9-e0e6-4175-8e5d-0025798add7b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a73afc59-f9b5-4e05-9324-8e2ed679a001'), datetime.datetime(2023, 3, 29, 15, 26, 26, 237159), datetime.datetime(2023, 3, 29, 15, 26, 26, 237159)), (UUID('ab677f9b-a376-49b0-8839-a8148afdbfe3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('650448e9-54f0-4bfa-8c87-33bc6d6fc6c1'), datetime.datetime(2023, 3, 29, 15, 26, 26, 616030), datetime.datetime(2023, 3, 29, 15, 26, 26, 616030)), (UUID('e28128ca-bc64-4ef8-ae0d-89eb2f73a925'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('794a7fdf-0ead-4c8f-8cc5-3836953df468'), datetime.datetime(2023, 3, 29, 15, 26, 27, 192959), datetime.datetime(2023, 3, 29, 15, 26, 27, 192959)), (UUID('5126d8e8-c868-4df9-8f53-1519803846ff'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e199fc27-0471-431c-b88c-0a22911571d3'), datetime.datetime(2023, 3, 31, 18, 14, 41, 387917), datetime.datetime(2023, 3, 31, 18, 14, 41, 387917)), (UUID('6e3e5459-27ee-45e0-9f98-4adcc712b227'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('55502546-e36a-4d60-8821-fd921adf594f'), datetime.datetime(2023, 4, 3, 22, 24, 7, 145312), datetime.datetime(2023, 4, 3, 22, 24, 7, 145312)), (UUID('1b0e46d3-22ea-43e1-87a1-98f6295c6a71'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('44aa025c-65d3-4010-8afb-96977a1bc29e'), datetime.datetime(2023, 4, 3, 22, 24, 8, 22180), datetime.datetime(2023, 4, 3, 22, 24, 8, 22180)), (UUID('99109fae-0885-4b6c-bb6d-05658dc8a999'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c4535b54-f7e3-40ab-91f9-cd61219de1ee'), datetime.datetime(2023, 4, 3, 22, 24, 8, 555104), datetime.datetime(2023, 4, 3, 22, 24, 8, 555104)), (UUID('6ec87149-6eb8-49f6-8c38-e8f98d198faa'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('8d5a09e9-366e-4927-89a8-2e1bff4cc8c4'), datetime.datetime(2023, 4, 4, 20, 25, 44, 844836), datetime.datetime(2023, 4, 4, 20, 25, 44, 844836)), (UUID('ab5c28a4-2bac-4290-ac2b-c40241da2b87'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5da5dc0b-2536-46fd-bafe-027ffb54048d'), datetime.datetime(2023, 4, 5, 21, 14, 25, 156348), datetime.datetime(2023, 4, 5, 21, 14, 25, 156348)), (UUID('5c06d5e2-796a-49df-95e0-ad7ca38fb39d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('8574be2f-a250-4774-bd42-a237dafa2d30'), datetime.datetime(2023, 4, 11, 22, 28, 11, 55052), datetime.datetime(2023, 4, 11, 22, 28, 11, 55052)), (UUID('aa4a9292-455c-4e43-b716-a65a73e8f805'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5a442fc3-695c-4e50-9629-f9301ffe0782'), datetime.datetime(2023, 4, 11, 22, 28, 11, 829390), datetime.datetime(2023, 4, 11, 22, 28, 11, 829390)), (UUID('631db80f-ba3b-489c-a832-a6a2e7ae2f08'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('27eb5922-1fe1-44fc-bdf7-58409b91209f'), datetime.datetime(2023, 4, 11, 22, 28, 12, 523119), datetime.datetime(2023, 4, 11, 22, 28, 12, 523119)), (UUID('c852a95c-8720-4f54-81d0-fe3c78f942fb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c7cb8c4b-a581-4b23-95ac-bc17097049b1'), datetime.datetime(2023, 4, 11, 22, 28, 13, 13739), datetime.datetime(2023, 4, 11, 22, 28, 13, 13739)), (UUID('aaeb1fc7-94f3-4525-864d-7b377b3cab93'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('893acc25-3d3b-48ec-829c-3ee0be3c9e3c'), datetime.datetime(2023, 4, 11, 22, 28, 13, 776591), datetime.datetime(2023, 4, 11, 22, 28, 13, 776591)), (UUID('3f49ba21-6086-4f66-a9cb-75d0e787f36d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('0011f528-8d83-40af-9fff-5ac660df38ed'), datetime.datetime(2023, 4, 11, 22, 28, 14, 250207), datetime.datetime(2023, 4, 11, 22, 28, 14, 250207)), (UUID('af9b2b0c-8db9-41db-9d82-e4143fe188e3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('53019c43-fdda-4e47-a315-333e6827663f'), datetime.datetime(2023, 4, 11, 22, 28, 14, 771797), datetime.datetime(2023, 4, 11, 22, 28, 14, 771797)), (UUID('ef1e38b0-7c4e-40f2-9354-b9534fa52521'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('b4978c98-094f-41ab-9a13-2e133ed275cb'), datetime.datetime(2023, 4, 11, 22, 28, 15, 393175), datetime.datetime(2023, 4, 11, 22, 28, 15, 393175)), (UUID('4423c765-83bd-4d0d-84c9-e4b9c6d2d571'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1a1dc33f-c8d3-4b06-b234-65e541d7f4f3'), datetime.datetime(2023, 4, 12, 22, 4, 50, 219495), datetime.datetime(2023, 4, 12, 22, 4, 50, 219495)), (UUID('e4b99583-2182-4f12-a8b3-ba6bd09b680e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3a672e1a-760f-4a3f-9257-f7e8edd5fe40'), datetime.datetime(2023, 4, 17, 13, 24, 32, 886087), datetime.datetime(2023, 4, 17, 13, 24, 32, 886087)), (UUID('bc26ea75-606b-4783-ab93-b5c49fc34fe3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a619dc52-44ea-4f9a-bcbc-543f9aeca80b'), datetime.datetime(2023, 4, 20, 17, 3, 8, 77347), datetime.datetime(2023, 4, 20, 17, 3, 8, 77347)), (UUID('c3420421-18f2-44b5-a3fb-619283f27c76'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('83413ec7-dd62-4614-8a04-e88388e04e30'), datetime.datetime(2023, 4, 24, 12, 41, 57, 496744), datetime.datetime(2023, 4, 24, 12, 41, 57, 496744)), (UUID('bf7b8610-3744-4fe8-a18f-bf7e357a1268'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5d27d3e1-7737-498e-bdb3-7420a2c0c61a'), datetime.datetime(2023, 4, 26, 22, 54, 3, 790085), datetime.datetime(2023, 4, 26, 22, 54, 3, 790085)), (UUID('25599bf6-0e50-4bde-818a-fdfc5d23cf85'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a0f0679d-05cd-4cb4-8dc1-729232ffedff'), datetime.datetime(2023, 4, 26, 22, 54, 5, 35726), datetime.datetime(2023, 4, 26, 22, 54, 5, 35726)), (UUID('aa146943-2154-48c3-8b0a-32aa68c4bf5a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2a2c6dc1-58b4-475f-9ee0-6e47a9457b23'), datetime.datetime(2023, 4, 27, 21, 18, 25, 588912), datetime.datetime(2023, 4, 27, 21, 18, 25, 588912)), (UUID('8810ed2e-9a18-4195-b400-fdaa4219b7d3'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2eaa1d1e-5196-4a68-9261-61680ec0433b'), datetime.datetime(2023, 4, 27, 21, 18, 26, 40215), datetime.datetime(2023, 4, 27, 21, 18, 26, 40215)), (UUID('e082a8a5-4540-49a6-b73c-28be5ea7655b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('56b69264-fe0f-4673-b603-87e6e47f9542'), datetime.datetime(2023, 4, 27, 21, 18, 26, 506701), datetime.datetime(2023, 4, 27, 21, 18, 26, 506701)), (UUID('2fbb947d-734a-438f-b767-82926ac560ff'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('85a28e2e-d7b8-4f05-89bd-70ea14a80757'), datetime.datetime(2023, 4, 27, 21, 18, 26, 955550), datetime.datetime(2023, 4, 27, 21, 18, 26, 955550)), (UUID('92f470a9-bbd7-4070-8160-282224b3b145'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f24dae89-a63c-4507-9372-33c8da0c89d4'), datetime.datetime(2023, 4, 28, 6, 41, 24, 565317), datetime.datetime(2023, 4, 28, 6, 41, 24, 565317)), (UUID('3dbe9e64-2059-4703-a4bb-c649278885cf'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('967cbe41-515d-43fd-a9e1-eb1cbd5320c4'), datetime.datetime(2023, 5, 2, 15, 46, 36, 430876), datetime.datetime(2023, 5, 2, 15, 46, 36, 430876)), (UUID('9963a149-4da5-4c9d-bf56-90d36a56e390'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('8cc4ac84-42f5-45cc-a6fa-d976d69cc822'), datetime.datetime(2023, 5, 2, 15, 46, 37, 202949), datetime.datetime(2023, 5, 2, 15, 46, 37, 202949)), (UUID('2bce7673-fb75-42cf-be21-832a5dfcc6ab'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('179fc18f-7c8f-4847-855c-849679b41445'), datetime.datetime(2023, 5, 3, 20, 33, 42, 829192), datetime.datetime(2023, 5, 3, 20, 33, 42, 829192)), (UUID('f2db682a-38c1-4420-b174-d4670db9d3c1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('fb7d7001-39fc-4df1-9090-eb0254d464fa'), datetime.datetime(2023, 5, 3, 20, 33, 43, 267784), datetime.datetime(2023, 5, 3, 20, 33, 43, 267784)), (UUID('e1f0ff13-ac7a-4036-a941-c8a10962692b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7fe7e951-0444-4d47-8de4-d45446392f5a'), datetime.datetime(2023, 5, 3, 20, 33, 43, 796517), datetime.datetime(2023, 5, 3, 20, 33, 43, 796517)), (UUID('757f1e38-2161-48b2-8e6f-20d2aa284d4c'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('8f4ed0c3-822d-4c7e-a18c-f63463c7f15f'), datetime.datetime(2023, 5, 3, 20, 33, 44, 332180), datetime.datetime(2023, 5, 3, 20, 33, 44, 332180)), (UUID('6704c600-3cb2-4500-9e0d-a1ac25168bfc'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4fa02d84-f130-4406-b960-518f43e91f25'), datetime.datetime(2023, 7, 14, 15, 29, 30, 667410), datetime.datetime(2023, 7, 14, 15, 29, 30, 667410)), (UUID('2cec2c42-cc82-4988-be81-88f307102d0b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('94f3d06b-d907-4fc8-87f8-c31c057ea5e7'), datetime.datetime(2023, 7, 19, 20, 45, 30, 403526), datetime.datetime(2023, 7, 19, 20, 45, 30, 403526)), (UUID('4560fa7b-bc09-441c-9d18-c8dc65b87543'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('857b1f69-92f4-490a-ab36-201a6135b6ab'), datetime.datetime(2023, 7, 20, 17, 53, 4, 991627), datetime.datetime(2023, 7, 20, 17, 53, 4, 991627)), (UUID('144f6d28-6e55-46ce-814b-8eb5d86d90c2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('45c6fd2d-1291-4c27-8bdc-cd7d07cee21a'), datetime.datetime(2023, 7, 28, 18, 48, 12, 730538), datetime.datetime(2023, 7, 28, 18, 48, 12, 730538)), (UUID('0c2a9553-c4a7-4092-bcaa-eeebf6877bb4'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2f8b2600-e63c-4ed6-ab1e-5c238932bbf5'), datetime.datetime(2023, 7, 28, 18, 48, 19, 361922), datetime.datetime(2023, 7, 28, 18, 48, 19, 361922)), (UUID('5b7fa7ec-7cf9-4611-bba6-272f756b6214'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('0874c709-e5b0-4cce-bc30-b8c986cd27bc'), datetime.datetime(2023, 8, 15, 17, 28, 33, 744378), datetime.datetime(2023, 8, 15, 17, 28, 33, 744378)), (UUID('aecc0ee7-67cb-4a7b-855a-337f78758479'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('81f62c77-7291-4ae9-818a-99f43821c5fc'), datetime.datetime(2023, 5, 3, 20, 33, 44, 831884), datetime.datetime(2023, 5, 3, 20, 33, 44, 831884)), (UUID('b5e3751a-8618-41e4-84a7-9618190a27d2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('9fe563f8-945d-4f47-9d57-70f61a6a560f'), datetime.datetime(2023, 5, 3, 20, 33, 46, 250323), datetime.datetime(2023, 5, 3, 20, 33, 46, 250323)), (UUID('0c02b194-2116-447b-a98d-7957cc1fcbee'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d7436587-3972-4da1-a3fc-0d205482eddd'), datetime.datetime(2023, 5, 5, 3, 17, 12, 290340), datetime.datetime(2023, 5, 5, 3, 17, 12, 290340)), (UUID('1586c16c-5bac-4f00-9274-05385aca24bc'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('9edbf21c-3b76-4e83-a449-00915ba4e8c2'), datetime.datetime(2023, 5, 5, 3, 17, 12, 841241), datetime.datetime(2023, 5, 5, 3, 17, 12, 841241)), (UUID('7ec4dca8-2949-48a1-8876-1cada6e7702d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('273078bc-5da5-4910-a432-d3f45d8dd4d3'), datetime.datetime(2023, 5, 5, 3, 17, 13, 770722), datetime.datetime(2023, 5, 5, 3, 17, 13, 770722)), (UUID('a48c4182-7c02-4f0e-98f2-1bbb41c46870'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('75242c33-f249-4417-ad86-7691bb888b40'), datetime.datetime(2023, 5, 11, 13, 41, 15, 306136), datetime.datetime(2023, 5, 11, 13, 41, 15, 306136)), (UUID('2642ebfa-8ebb-4738-8946-69d0658588de'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7ca04334-7a02-4f71-86da-29cd332ecd58'), datetime.datetime(2023, 5, 15, 16, 24, 44, 262708), datetime.datetime(2023, 5, 15, 16, 24, 44, 262708)), (UUID('9f0e169f-6119-44ce-a6d6-c69c53c60ddd'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('040a2004-1232-4341-941e-1e0010764d92'), datetime.datetime(2023, 5, 15, 16, 35, 25, 581585), datetime.datetime(2023, 5, 15, 16, 35, 25, 581585)), (UUID('8e8ef18a-a859-47e0-99c8-7d05a219adeb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e1f9c43b-196e-4cfc-8592-cfe311fdb9ad'), datetime.datetime(2023, 5, 15, 16, 35, 26, 218025), datetime.datetime(2023, 5, 15, 16, 35, 26, 218025)), (UUID('8e77ee66-8855-414d-8e7e-93c89be62d76'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('b9f7ca35-056c-49ec-830f-862d7df9f0e4'), datetime.datetime(2023, 7, 14, 16, 23, 44, 864962), datetime.datetime(2023, 7, 14, 16, 23, 44, 864962)), (UUID('d73eb095-8a66-4a8f-abb8-c4b8f20ad6b5'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5c1191a0-ec5b-416d-a546-4507b3d17393'), datetime.datetime(2023, 7, 20, 17, 53, 5, 753846), datetime.datetime(2023, 7, 20, 17, 53, 5, 753846)), (UUID('8b5201f7-52c6-4c80-a735-f8d7eb599c46'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('9061a821-1666-4a84-98d9-bbe75f91c724'), datetime.datetime(2023, 7, 28, 13, 7, 56, 140698), datetime.datetime(2023, 7, 28, 13, 7, 56, 140698)), (UUID('66a4b875-527d-42ca-888a-ef062cf5a841'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e1dc5a50-73b8-4b99-85e7-e0efd6546ecd'), datetime.datetime(2023, 7, 28, 18, 48, 0, 364120), datetime.datetime(2023, 7, 28, 18, 48, 0, 364120)), (UUID('589e7211-e136-4064-8457-f8ff8e49c7eb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('412833ba-10d9-4424-8efb-630ce86e32c6'), datetime.datetime(2023, 7, 28, 18, 48, 13, 367588), datetime.datetime(2023, 7, 28, 18, 48, 13, 367588)), (UUID('04bc5fd0-906a-4c0b-a35e-31ad633bc127'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('082f732f-34e5-4909-8a7c-f90e40ea2fef'), datetime.datetime(2023, 7, 28, 18, 48, 15, 774351), datetime.datetime(2023, 7, 28, 18, 48, 15, 774351)), (UUID('15bacddb-00f2-44d1-9562-78fb5b0775ae'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3266ca88-48a9-4cfb-873d-2bf45253bc6a'), datetime.datetime(2023, 5, 15, 16, 35, 26, 837430), datetime.datetime(2023, 5, 15, 16, 35, 26, 837430)), (UUID('4ee1fd47-2e04-4a4f-bb52-ecb89e8a8d43'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('981e70d2-1b54-4a7f-8fd2-1abbdf19e18e'), datetime.datetime(2023, 7, 14, 14, 8, 21, 725550), datetime.datetime(2023, 7, 14, 14, 8, 21, 725550)), (UUID('688184a8-6051-49f9-a934-46a07ea0fe8e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a4f19cf5-6ad8-421a-9d63-dc6b3f842e13'), datetime.datetime(2023, 7, 18, 13, 40, 25, 502718), datetime.datetime(2023, 7, 18, 13, 40, 25, 502718)), (UUID('c6e6ea42-2e28-42bb-9b89-b7fe47b11f33'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('54787c7a-fad5-48ce-b5c2-4f9295b88ecd'), datetime.datetime(2023, 7, 20, 17, 53, 6, 337774), datetime.datetime(2023, 7, 20, 17, 53, 6, 337774)), (UUID('cec17dc9-8e8a-44d2-93a0-583c3d5fe2ac'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('bda0e7a7-933b-44c0-8a48-d0a28ee20072'), datetime.datetime(2023, 7, 28, 18, 48, 13, 997641), datetime.datetime(2023, 7, 28, 18, 48, 13, 997641)), (UUID('57eae69b-96f7-40d8-952b-ef5b9d4b385d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('4e718579-262c-44d4-b808-0eb12e3af80c'), datetime.datetime(2023, 7, 28, 18, 48, 16, 832332), datetime.datetime(2023, 7, 28, 18, 48, 16, 832332)), (UUID('5316d111-47ad-4082-8d95-ff6ffc91eeab'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1c634eb7-41e7-452b-8313-77083f38d8b7'), datetime.datetime(2023, 7, 28, 18, 48, 18, 447206), datetime.datetime(2023, 7, 28, 18, 48, 18, 447206)), (UUID('bb42e871-43bc-495e-8c69-f73acef913c5'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d9c12347-b4f7-42be-94c2-c4412d8ecca7'), datetime.datetime(2023, 8, 7, 20, 27, 26, 978134), datetime.datetime(2023, 8, 7, 20, 27, 26, 978134)), (UUID('fb1b560b-1961-4f48-a70a-6d6b668484e1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('bb5e0154-837e-4118-adb0-f81dc0580ae5'), datetime.datetime(2023, 5, 15, 16, 35, 27, 340373), datetime.datetime(2023, 5, 15, 16, 35, 27, 340373)), (UUID('24978512-b0ca-4b77-9da3-1f7be3e956de'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('35b9dff2-25d3-4c08-847b-45b49397b847'), datetime.datetime(2023, 5, 18, 11, 2, 56, 63107), datetime.datetime(2023, 5, 18, 11, 2, 56, 63107)), (UUID('0ca20377-51a2-430c-ac4c-62f3f6ebded0'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('7934457e-4e6e-4a3d-b172-528cd0f7a541'), datetime.datetime(2023, 5, 25, 13, 45, 58, 465729), datetime.datetime(2023, 5, 25, 13, 45, 58, 465729)), (UUID('5a6c4daf-2c99-459b-a7c7-2aaef58399dc'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ab45cade-b750-4dd1-a035-7715f65b2dc0'), datetime.datetime(2023, 5, 25, 13, 45, 58, 854345), datetime.datetime(2023, 5, 25, 13, 45, 58, 854345)), (UUID('efe75984-b489-417c-bc64-63d3a93bcb04'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('21142656-af9f-4423-b824-b7215c29e1c5'), datetime.datetime(2023, 5, 25, 13, 45, 59, 300675), datetime.datetime(2023, 5, 25, 13, 45, 59, 300675)), (UUID('2bb77344-8bf4-48c1-9c99-be83d542cff1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d311c959-f3c9-495b-a6d0-4240bfe73fcd'), datetime.datetime(2023, 5, 25, 13, 45, 59, 821342), datetime.datetime(2023, 5, 25, 13, 45, 59, 821342)), (UUID('5a9a622c-de30-41ef-bc6f-c71efea402b9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('2305fbe9-348a-43fb-8f04-b7a39a3da8c0'), datetime.datetime(2023, 5, 31, 18, 43, 47, 563466), datetime.datetime(2023, 5, 31, 18, 43, 47, 563466)), (UUID('84431f46-f356-43b3-b75d-78569c2db10e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a22a6765-9a67-41dc-a314-4bf2445ee10c'), datetime.datetime(2023, 5, 31, 18, 43, 48, 104826), datetime.datetime(2023, 5, 31, 18, 43, 48, 104826)), (UUID('afe2887d-921d-4ca9-ad7b-4ecfae5d07cb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('02a48ac9-b620-4d6c-a9ab-f62d353e1258'), datetime.datetime(2023, 5, 31, 18, 43, 48, 636795), datetime.datetime(2023, 5, 31, 18, 43, 48, 636795)), (UUID('49795af3-61a6-4a19-84dd-4cff96aa4a6c'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ea80f655-176a-4193-adee-4e397d8995b3'), datetime.datetime(2023, 5, 31, 20, 40, 28, 908519), datetime.datetime(2023, 5, 31, 20, 40, 28, 908519)), (UUID('ce17eaa2-f071-42ac-8ef6-378ccf34497d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f1cdda3a-3ce8-492a-8cfc-f09b3720a352'), datetime.datetime(2023, 5, 31, 20, 40, 29, 551293), datetime.datetime(2023, 5, 31, 20, 40, 29, 551293)), (UUID('b54ee5ad-356f-4695-a8f8-16398c020a3d'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('45d966db-1f61-47c3-ab9f-3fd46f7aa7b5'), datetime.datetime(2023, 5, 31, 20, 40, 30, 224094), datetime.datetime(2023, 5, 31, 20, 40, 30, 224094)), (UUID('bc45f95d-ea96-48ea-b839-f2a13d897b6e'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('f3774323-f31f-4b86-9b8a-93bf3e70d5e0'), datetime.datetime(2023, 6, 2, 14, 43, 59, 750155), datetime.datetime(2023, 6, 2, 14, 43, 59, 750155)), (UUID('c4c833f6-ed88-48bc-8684-cf7b12e27970'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d2a6aa1a-85b2-424b-8cd0-39382e4bc0f0'), datetime.datetime(2023, 6, 2, 14, 44, 0, 246852), datetime.datetime(2023, 6, 2, 14, 44, 0, 246852)), (UUID('e14b07bb-d9d2-410d-a458-3a2111208d64'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('81afee03-8ba6-4d9f-b716-29c5e7525f0e'), datetime.datetime(2023, 6, 6, 19, 20, 58, 224910), datetime.datetime(2023, 6, 6, 19, 20, 58, 224910)), (UUID('153ae709-345c-4b86-8222-c57193b8fbcb'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('81fee1cf-7fac-4ee1-ba06-f224cee31f99'), datetime.datetime(2023, 6, 6, 19, 20, 58, 646017), datetime.datetime(2023, 6, 6, 19, 20, 58, 646017)), (UUID('57f6768e-9d89-4799-8067-738f11f7d607'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('311c9038-ed8b-482c-9058-5f6b2beccecb'), datetime.datetime(2023, 6, 7, 12, 48, 12, 689723), datetime.datetime(2023, 6, 7, 12, 48, 12, 689723)), (UUID('43ff5bdc-97da-4061-91e1-5b062fd45253'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5c954ee4-8dd7-4bdc-a3ce-760e2352b66f'), datetime.datetime(2023, 6, 7, 12, 48, 13, 116708), datetime.datetime(2023, 6, 7, 12, 48, 13, 116708)), (UUID('b57bfd13-62bb-4935-9d2b-7df54b016fde'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('45a4d20f-ebab-4f9f-9fc9-318f6c1b3b02'), datetime.datetime(2023, 7, 14, 14, 10, 2, 115679), datetime.datetime(2023, 7, 14, 14, 10, 2, 115679)), (UUID('68370437-010f-4fe8-9647-4ee076b53d28'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('05959cae-3940-4e6d-8a35-3c0544d3c6fc'), datetime.datetime(2023, 7, 19, 20, 38, 0, 939091), datetime.datetime(2023, 7, 19, 20, 38, 0, 939091)), (UUID('efa8ad47-8f3b-4832-9ca9-a0244562dbb2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('41d9f6c6-e74f-4d28-b141-3c75d7d9658a'), datetime.datetime(2023, 7, 28, 18, 48, 9, 128102), datetime.datetime(2023, 7, 28, 18, 48, 9, 128102)), (UUID('d57e734c-2b1e-44ed-99ea-ef73f552a0ab'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('fa9cdd3a-c59b-4a52-b42f-12011893db64'), datetime.datetime(2023, 7, 28, 18, 48, 14, 627522), datetime.datetime(2023, 7, 28, 18, 48, 14, 627522)), (UUID('053eb426-31f0-49fb-8ba7-04e1399f6257'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a838c804-a48e-4f83-a031-7acd05a96805'), datetime.datetime(2023, 7, 28, 18, 48, 17, 754858), datetime.datetime(2023, 7, 28, 18, 48, 17, 754858)), (UUID('0d8d911e-2544-4172-ba10-2b93115d619a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('ceb8b5da-f938-42e3-813c-b3d3a25f2b55'), datetime.datetime(2023, 8, 9, 18, 11, 12, 288726), datetime.datetime(2023, 8, 9, 18, 11, 12, 288726)), (UUID('3f0766e0-6436-4d6a-a927-387ef9cf19fc'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c8736908-7a7d-41ed-82ae-eb1634a0455e'), datetime.datetime(2023, 6, 9, 14, 16, 32, 479597), datetime.datetime(2023, 6, 9, 14, 16, 32, 479597)), (UUID('3fb90cb6-ba9d-4100-9221-b2aaca007dda'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('55936be2-7d98-4731-a95e-1c117eb8137c'), datetime.datetime(2023, 6, 14, 17, 7, 51, 993551), datetime.datetime(2023, 6, 14, 17, 7, 51, 993551)), (UUID('6f96c818-5c6e-4f6a-88d8-1fbfa2d24831'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('45105b33-7acb-4d32-a7a4-af9fb3c59ffe'), datetime.datetime(2023, 6, 14, 17, 7, 52, 583126), datetime.datetime(2023, 6, 14, 17, 7, 52, 583126)), (UUID('49d690a3-7513-41cc-a6a1-77a81812f647'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('867092f4-f941-4136-a424-8a25f4924642'), datetime.datetime(2023, 6, 15, 12, 53, 19, 977179), datetime.datetime(2023, 6, 15, 12, 53, 19, 977179)), (UUID('d075498c-f406-44bc-ae9c-731d7106a24f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d1ad9535-71cf-4c5b-800d-9ed5a722b57e'), datetime.datetime(2023, 6, 20, 13, 22, 55, 953712), datetime.datetime(2023, 6, 20, 13, 22, 55, 953712)), (UUID('adcb43c2-54ef-48f9-94ca-d39047fc8d91'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d28a14f8-2921-4e79-9b35-f46bc0f33be7'), datetime.datetime(2023, 6, 20, 13, 22, 56, 597349), datetime.datetime(2023, 6, 20, 13, 22, 56, 597349)), (UUID('be7909b0-9f63-4c0c-96b7-7f81591cd6af'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('12f608f0-91d4-4b8c-8ec5-966f338ff042'), datetime.datetime(2023, 6, 20, 13, 23, 40, 97360), datetime.datetime(2023, 6, 20, 13, 23, 40, 97360)), (UUID('c134c374-5ad5-42e8-9148-0869e8cd43fa'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('15b446cc-ebfb-4d0c-976c-1eae8c961928'), datetime.datetime(2023, 6, 21, 15, 46, 45, 307231), datetime.datetime(2023, 6, 21, 15, 46, 45, 307231)), (UUID('8e7a6a2c-2404-4119-b1d0-608b45a5f955'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('026cd297-d600-4c19-87a7-51405669c9ab'), datetime.datetime(2023, 6, 21, 15, 46, 45, 888982), datetime.datetime(2023, 6, 21, 15, 46, 45, 888982)), (UUID('119f4baf-8f0f-4168-92a0-59279c11134f'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('1ee537bd-3840-4a54-98eb-6123562a194f'), datetime.datetime(2023, 6, 21, 15, 46, 47, 7098), datetime.datetime(2023, 6, 21, 15, 46, 47, 7098)), (UUID('2b35638b-5489-4b7c-ab89-6d834e8ac238'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a3c1239d-b746-4d68-ab19-53f3f0a62902'), datetime.datetime(2023, 6, 22, 14, 6, 38, 44433), datetime.datetime(2023, 6, 22, 14, 6, 38, 44433)), (UUID('61127a04-9a7a-4870-9a1b-4e819c4afd30'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('465eeda3-1f0a-4709-aa6c-e2d6985e2abd'), datetime.datetime(2023, 6, 26, 20, 7, 58, 646561), datetime.datetime(2023, 6, 26, 20, 7, 58, 646561)), (UUID('70ec040e-d004-485f-86f8-15767c3649b2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('3a4c1716-df04-4afe-8331-ca9c1a3e1398'), datetime.datetime(2023, 6, 27, 14, 10, 9, 548975), datetime.datetime(2023, 6, 27, 14, 10, 9, 548975)), (UUID('72db9ffc-0686-438c-8af4-59a35a40ca9c'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('65ea6038-893b-4712-a0c8-ac2254e4176e'), datetime.datetime(2023, 6, 28, 15, 46, 10, 250499), datetime.datetime(2023, 6, 28, 15, 46, 10, 250499)), (UUID('b912f54f-9ce5-490e-b404-eadbf100dba9'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('5dd03811-67d8-4cac-b592-76f77343a1d7'), datetime.datetime(2023, 6, 30, 18, 42, 19, 476390), datetime.datetime(2023, 6, 30, 18, 42, 19, 476390)), (UUID('3364e83b-5467-4a8a-9834-360487d73fc2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('a316e688-6299-46d8-9de4-7b028d81a90b'), datetime.datetime(2023, 6, 30, 18, 42, 20, 81046), datetime.datetime(2023, 6, 30, 18, 42, 20, 81046)), (UUID('9730e069-790c-406d-97e4-a635f61554a2'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('44ea6e5b-8edd-490c-9f67-a80643cba812'), datetime.datetime(2023, 6, 30, 18, 42, 20, 820568), datetime.datetime(2023, 6, 30, 18, 42, 20, 820568)), (UUID('ac8b18c2-fbc6-4109-b801-afae4fe61a88'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('d7033931-7b18-45fe-8967-55cd5a8f3803'), datetime.datetime(2023, 7, 6, 21, 17, 47, 80711), datetime.datetime(2023, 7, 6, 21, 17, 47, 80711)), (UUID('7dd5c7d0-3e7e-4c82-b6c6-9c77a7a5ab5c'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('63ff32cf-6174-487b-9071-326f965904e5'), datetime.datetime(2023, 7, 6, 21, 17, 47, 775510), datetime.datetime(2023, 7, 6, 21, 17, 47, 775510)), (UUID('0e80e911-e9c6-487a-97aa-fffdd1387bd7'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('10b87607-8344-4481-8fd1-ebf346d209e6'), datetime.datetime(2023, 7, 6, 21, 17, 48, 496782), datetime.datetime(2023, 7, 6, 21, 17, 48, 496782)), (UUID('5f54e189-2af0-4fd1-9d0c-f58f73d123a0'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('c24dfd0b-4a84-4854-8ea2-9cfd6adaa139'), datetime.datetime(2023, 7, 10, 17, 28, 52, 7781), datetime.datetime(2023, 7, 10, 17, 28, 52, 7781)), (UUID('6a91c599-cc05-4404-b952-cb8539e4e93b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('df752d6a-2c31-411a-9eaa-8d8b252e2d8d'), datetime.datetime(2023, 7, 10, 17, 28, 52, 761645), datetime.datetime(2023, 7, 10, 17, 28, 52, 761645)), (UUID('b309de28-810f-4521-8228-febb29db141a'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('82b5fd2f-e21a-428b-a1e0-318d752a1903'), datetime.datetime(2023, 7, 11, 14, 42, 21, 591615), datetime.datetime(2023, 7, 11, 14, 42, 21, 591615)), (UUID('056421ac-2409-4e06-b01c-44a2b2db598b'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('e8d14c2e-41a1-4802-9fb8-5e9f481ff02a'), datetime.datetime(2023, 7, 11, 14, 43, 14, 955673), datetime.datetime(2023, 7, 11, 14, 43, 14, 955673)), (UUID('6a7015e3-6890-4f6e-83cd-deec37789db1'), 'org:admin', 'Default role for administrator of an Organization.\\n', UUID('720f1ff8-d26b-415e-9cf7-a911422805e3'), datetime.datetime(2023, 7, 14, 14, 11, 41, 727713), datetime.datetime(2023, 7, 14, 14, 11, 41, 727713))]\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 21528.24 ms\n",
      "llama_print_timings:      sample time =    42.08 ms /    58 runs   (    0.73 ms per token,  1378.36 tokens per second)\n",
      "llama_print_timings: prompt eval time = 22881.54 ms /   297 tokens (   77.04 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:        eval time = 11086.58 ms /    57 runs   (  194.50 ms per token,     5.14 tokens per second)\n",
      "llama_print_timings:       total time = 34087.04 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (34355) exceed context window of 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      3\u001b[0m template \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      4\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHow many users have the \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg:admin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m role?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:503\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    504\u001b[0m         _output_key\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    509\u001b[0m         _output_key\n\u001b[1;32m    510\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/agents/agent.py:1146\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1146\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1155\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1156\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/agents/agent.py:933\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    930\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/agents/agent.py:546\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 546\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:497\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    491\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    496\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:646\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    632\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         )\n\u001b[1;32m    634\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    635\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    636\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m     ]\n\u001b[0;32m--> 646\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:534\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    533\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:521\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    513\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 521\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    529\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    530\u001b[0m         )\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:1043\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1042\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/llamacpp.py:291\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    292\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    294\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    296\u001b[0m     ):\n\u001b[1;32m    297\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/llamacpp.py:344\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    343\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    345\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    346\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    347\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    348\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    349\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/llama_cpp/llama.py:947\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m    944\u001b[0m     llama_cpp\u001b[38;5;241m.\u001b[39mllama_reset_timings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx):\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (34355) exceed context window of 2048"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    [INST]\n",
    "    <<SYS>>\n",
    "    You are an assistant tasked with querying an Postgresql database for information.\n",
    "    Given an input question, first create a syntactically correct postgresql query to run,  \n",
    "    then look at the results of the query and return the answer.\n",
    "    Only return the requested information from the database.\n",
    "    \n",
    "    The valid SQL query tools are:\n",
    "     1. sql_db_list_tables\n",
    "     2. sql_db_schema\n",
    "     3. sql_db_query_checker\n",
    "     4. sql_db_query\n",
    "     \n",
    "    For SQL queries, ALWAYS use the available tools in this order:\n",
    "     1. sql_db_list_tables\n",
    "     2. sql_db_schema\n",
    "     3. sql_db_query_checker\n",
    "     4. sql_db_query\n",
    "    <</SYS>>\n",
    "    \n",
    "    {question}\n",
    "    [/INST]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "agent_executor.run(template.format(question='How many users have the \"org:admin\" role?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89cf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
